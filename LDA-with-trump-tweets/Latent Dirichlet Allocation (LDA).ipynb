{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is a quick and dirty projet using Latent Dirichlet Allocation (LDA) to observe Donald Trump's tweets.    \n",
    "Project objectives include:    \n",
    "    - observe topics in Donald Trump's tweets\n",
    "    - identify (if any) differences in the tweets\n",
    "    - see if LDA models can effectively categorize tweets based on the their topics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. import, select, preview tweets data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('DT.csv', error_bad_lines=False);\n",
    "texts = data[['Tweet_Text']]\n",
    "texts['index'] = texts.index\n",
    "documents = texts\n",
    "print(len(documents))\n",
    "print(documents[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. pre-process tweets data to get clean corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/Eddie/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(1357)\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x):\n",
    "    output = []\n",
    "    lemStem = stemmer.stem(WordNetLemmatizer().lemmatize(x, pos='v'))\n",
    "    #lem = WordNetLemmatizer().lemmatize(x, pos='v')\n",
    "    #Stem = SnowballStemmer().stem(lem, \"english\")\n",
    "    for wd in gensim.utils.simple_preprocess(lemStem):\n",
    "        if wd not in gensim.parsing.preprocessing.STOPWORDS:\n",
    "            output.append(wd)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "['\"@nbcsnl:', 'Were', 'live', 'from', 'Studio', '8H', 'tonight.', '#SNL', 'https://t.co/Uiq9llzwEC\"']\n",
      "\n",
      "\n",
      " pre-processed document: \n",
      "['nbcsnl', 'live', 'studio', 'tonight', 'snl', 'https', 'uiq', 'llzwec']\n"
     ]
    }
   ],
   "source": [
    "# pick an example and test how preprocess() function is doing\n",
    "\n",
    "test_doc = documents[documents['index'] == 5000].values[0][0]\n",
    "print('original document: ')\n",
    "words = []\n",
    "for word in test_doc.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n pre-processed document: ')\n",
    "print(preprocess(test_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [today, express, deepest, gratitude, served, a...\n",
       "1     [busy, day, planned, new, york, soon, making, ...\n",
       "2     [love, fact, small, groups, protesters, night,...\n",
       "3     [open, successful, presidential, election, pro...\n",
       "4     [fantastic, day, met, president, obama, time, ...\n",
       "5     [happy, st, birthday, marine, corps, thank, se...\n",
       "6     [beautiful, important, evening, forgotten, man...\n",
       "7     [watching, returns, pm, electionnight, maga__,...\n",
       "8     [rt, ivankatrump, surreal, moment, vote, fathe...\n",
       "9     [rt, erictrump, join, family, incredible, move...\n",
       "10    [rt, donaldjtrumpjr, final, push, eric, dozens...\n",
       "11    [time, votetrump, ivoted, electionnight, https...\n",
       "12    [dont, let, getting, vote, election, far, time...\n",
       "13    [according, cnn, utah, officials, report, voti...\n",
       "14    [watching, election, results, trump, tower, ma...\n",
       "15    [electionday, https, mxraxyntjy, https, fzhoncih]\n",
       "16    [need, vote, polls, lets, continue, movement, ...\n",
       "17    [vote, today, https, mxraxyntjy, polling, loca...\n",
       "18                              [today, america, great]\n",
       "19    [today, going, win, great, state, michigan, go...\n",
       "20    [rt, erictrump, sean, hannity, hillary, wins, ...\n",
       "21    [rt, donaldjtrumpjr, thanks, new, hampshire, n...\n",
       "22    [rt, detroitnews, ivankatrump, michigan, movem...\n",
       "23    [unbelievable, evening, new, hampshire, thank,...\n",
       "24    [big, news, share, new, hampshire, tonight, po...\n",
       "25    [today, florida, pledged, stand, people, cuba,...\n",
       "26    [thank, pennsylvania, going, new, hampshire, m...\n",
       "27    [live, periscope, join, minutes, pennsylvania,...\n",
       "28    [hey, missouri, lets, defeat, crooked, hillary...\n",
       "29    [america, decide, failed, policies, fresh, per...\n",
       "30          [like, trump, need, vote, https, rvuduehzq]\n",
       "31                          [trump, https, rpwiyb, aov]\n",
       "32    [love, north, carolina, thank, amazing, suppor...\n",
       "33                                    [way, https, ild]\n",
       "34    [landed, north, carolina, heading, dorton, are...\n",
       "35    [starting, tomorrow, going, americafirst, than...\n",
       "36    [thank, support, virginia, day, votetrumppence...\n",
       "37    [thank, pennsylvania, forever, grateful, amazi...\n",
       "38    [thank, michigan, movement, seen, chance, drai...\n",
       "39    [american, comeback, story, begins, america, s...\n",
       "40    [thank, minnesota, time, draintheswamp, amp, m...\n",
       "41    [monday, scranton, pennsylvania, pm, https, bc...\n",
       "42    [thank, iowa, amp, votetrumppence, https, hfih...\n",
       "43    [rt, ivankatrump, thank, new, hampshire, https...\n",
       "44    [van, jones, crack, blue, wall, trade, https, ...\n",
       "45    [great, night, denver, colorado, thank, americ...\n",
       "46    [rt, danscavino, join, realdonaldtrump, live, ...\n",
       "47    [thank, reno, nevada, stop, quest, america, sa...\n",
       "48    [join, live, reno, nevada, https, bf, hrxaa, h...\n",
       "49    [join, tomorrow, minnesota, pm, https, wcglh, ...\n",
       "Name: Tweet_Text, dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dump all the tweets into the preprocess() function and store the results\n",
    "\n",
    "processed_docs = documents['Tweet_Text'].map(preprocess)\n",
    "processed_docs[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 armed\n",
      "1 deepest\n",
      "2 express\n",
      "3 forces\n",
      "4 gratitude\n",
      "5 https\n",
      "6 qwpk\n",
      "7 served\n",
      "8 thankavet\n",
      "9 today\n",
      "10 wpk\n",
      "11 busy\n",
      "12 day\n",
      "13 decisions\n",
      "14 government\n",
      "15 important\n",
      "16 making\n",
      "17 new\n",
      "18 people\n",
      "19 planned\n",
      "20 running\n"
     ]
    }
   ],
   "source": [
    "# use gensim to create a token dictionary\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. set threshholds to filter tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter extreme tokens. For rare tokens that appear fewer than 10 times and those that appear over 60% of the corpus size\n",
    "# in the end, we will keep 10,000 most common tokens\n",
    "dictionary.filter_extremes(no_below=10, no_above=0.6, keep_n=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. get bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 5 (\"https\") occurs 1 time.\n",
      "Word 153 (\"hillary\") occurs 1 time.\n",
      "Word 469 (\"total\") occurs 1 time.\n",
      "Word 769 (\"to_\") occurs 1 time.\n",
      "Word 785 (\"said\") occurs 1 time.\n",
      "Word 922 (\"fit\") occurs 1 time.\n",
      "Word 923 (\"hbirgj\") occurs 1 time.\n",
      "Word 924 (\"lie\") occurs 1 time.\n",
      "Word 925 (\"sniper\") occurs 1 time.\n",
      "Word 926 (\"surrounded\") occurs 1 time.\n",
      "Word 927 (\"turned\") occurs 1 time.\n",
      "Word 928 (\"usss\") occurs 1 time.\n"
     ]
    }
   ],
   "source": [
    "# use doc2bow() to get the frequencies of each unique token in each document (tweet)\n",
    "\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "\n",
    "# bow_corpus[168]\n",
    "# use no.168 tweet for sanity check  \n",
    "doc168bow = bow_corpus[167] # since python index starts with 0, the index is 167 for no.168 tweet\n",
    "for i in range(len(doc168bow)):\n",
    "    print(\"Word {} (\\\"{}\\\") occurs {} time.\".format(doc168bow[i][0], \n",
    "                                               dictionary[doc168bow[i][0]], doc168bow[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. get TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.30855648236230654),\n",
      " (1, 0.30855648236230654),\n",
      " (2, 0.32036118979895833),\n",
      " (3, 0.32036118979895833),\n",
      " (4, 0.33699899046618453),\n",
      " (5, 0.048314725767360775),\n",
      " (6, 0.3654414985700625),\n",
      " (7, 0.2919186816950804),\n",
      " (8, 0.33699899046618453),\n",
      " (9, 0.15013581908492357),\n",
      " (10, 0.3654414985700625)]\n",
      "[(11, 0.4384947196000289),\n",
      " (12, 0.22124023855411182),\n",
      " (13, 0.37160688989712864),\n",
      " (14, 0.3155545278400487),\n",
      " (15, 0.30581997026715957),\n",
      " (16, 0.2605808337198211),\n",
      " (17, 0.15600760174699135),\n",
      " (18, 0.15274698884137305),\n",
      " (19, 0.3474743885816719),\n",
      " (20, 0.2710836383151232),\n",
      " (21, 0.22856571470692552),\n",
      " (22, 0.2560146127214273)]\n",
      "[(23, 0.2955464303737287),\n",
      " (24, 0.21500065508005778),\n",
      " (25, 0.32195874482687165),\n",
      " (26, 0.11920656293060786),\n",
      " (27, 0.3629808160169991),\n",
      " (28, 0.22745288696870547),\n",
      " (29, 0.22277342683399368),\n",
      " (30, 0.4040028872071265),\n",
      " (31, 0.37526939922514985),\n",
      " (32, 0.286762587364972),\n",
      " (33, 0.3593929091167368)]\n"
     ]
    }
   ],
   "source": [
    "# build TF-IDF object using bow_corpus\n",
    "\n",
    "from gensim import corpora, models\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "from pprint import pprint\n",
    "count = 0\n",
    "\n",
    "# print out the first 3 documents to see their TF-IDF\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    count += 1\n",
    "    if count >= 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. LDA model using bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: [('https', 0.022474874), ('trump', 0.013916188), ('http', 0.012599999), ('thank', 0.011988601), ('great', 0.009230257), ('realdonaldtrump', 0.008889767), ('rt', 0.007360429), ('cnn', 0.0068106838), ('america', 0.006691999), ('vote', 0.006301517)]\n",
      "Topic: 1 \n",
      "Words: [('trump', 0.02320403), ('https', 0.022528851), ('new', 0.010681709), ('thank', 0.00826124), ('realdonaldtrump', 0.007611265), ('great', 0.0066496427), ('poll', 0.006541915), ('people', 0.0063205766), ('tonight', 0.006231952), ('amp', 0.0055969083)]\n",
      "Topic: 2 \n",
      "Words: [('realdonaldtrump', 0.04254456), ('https', 0.033187404), ('trump', 0.01799937), ('amp', 0.013525866), ('great', 0.012807073), ('thank', 0.008255979), ('america', 0.0063346014), ('rt', 0.0061293403), ('hillary', 0.0061226757), ('http', 0.004500832)]\n",
      "Topic: 3 \n",
      "Words: [('trump', 0.034351584), ('realdonaldtrump', 0.027367981), ('great', 0.023072526), ('https', 0.017378042), ('thank', 0.013730368), ('america', 0.009406475), ('cnn', 0.008115207), ('http', 0.007100861), ('rt', 0.0065118545), ('people', 0.005774272)]\n",
      "Topic: 4 \n",
      "Words: [('https', 0.038033035), ('trump', 0.02245096), ('hillary', 0.010427029), ('donald', 0.008486915), ('clinton', 0.006154109), ('http', 0.0060770637), ('like', 0.0054709194), ('crooked', 0.004325816), ('president', 0.003954195), ('jeb', 0.0038763469)]\n"
     ]
    }
   ],
   "source": [
    "# LDA w/ bag of words. Use topic number = 5 without a strong assumption\n",
    "\n",
    "lda_model_bagwd = gensim.models.LdaMulticore(bow_corpus, num_topics=5, id2word=dictionary, passes=2, workers=2)\n",
    "\n",
    "for idx, topic in lda_model_bagwd.show_topics(formatted=False, num_words= 10):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. LDA model using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0\n",
      " Word: [('trump', 0.0055621434), ('https', 0.0054579587), ('realdonaldtrump', 0.0049422355), ('great', 0.0043380414), ('thank', 0.0043010963), ('poll', 0.0030632033), ('http', 0.0028839065), ('rt', 0.0028728626), ('america', 0.0026202595), ('new', 0.0024704987)]\n",
      "Topic: 1\n",
      " Word: [('https', 0.005192176), ('trump', 0.0044930703), ('realdonaldtrump', 0.00406542), ('thank', 0.003941524), ('great', 0.0038001095), ('amp', 0.002685054), ('america', 0.0025206378), ('people', 0.00249638), ('tonight', 0.002177804), ('cnn', 0.0021590334)]\n",
      "Topic: 2\n",
      " Word: [('https', 0.0052454895), ('trump', 0.004629123), ('thank', 0.003722043), ('realdonaldtrump', 0.0034826875), ('great', 0.0030130579), ('amp', 0.0027811544), ('america', 0.0023733608), ('people', 0.0018757227), ('hillary', 0.0018369337), ('donald', 0.0016012391)]\n",
      "Topic: 3\n",
      " Word: [('interviewed', 0.0049579735), ('https', 0.0041542416), ('enjoy', 0.0033767347), ('realdonaldtrump', 0.003281778), ('trump', 0.0032197528), ('foxandfriends', 0.0026759086), ('hillary', 0.0026543422), ('president', 0.0025879731), ('foxnews', 0.0025414277), ('thank', 0.0025402943)]\n",
      "Topic: 4\n",
      " Word: [('great', 0.005336999), ('https', 0.0049131634), ('trump', 0.0042409254), ('realdonaldtrump', 0.004223965), ('america', 0.0040530376), ('hillary', 0.0029002547), ('thank', 0.002741308), ('clinton', 0.0025528972), ('enjoy', 0.0022433356), ('rt', 0.0022310622)]\n"
     ]
    }
   ],
   "source": [
    "# LDA w/ TF-IDF\n",
    "\n",
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=5, id2word=dictionary, passes=2, workers=4)\n",
    "for idx, topic in lda_model_tfidf.show_topics(formatted=False, num_words= 10):\n",
    "    print('Topic: {}\\n Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Both bag of words and TF-IDF models show that there is a large homogeneity between topics and the word they consist of. This indicate that using single person's tweets may not be the best practice for LDA since the tweets inevitably contains very similar information rather than distinct topics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Test TF-IDF LDA model using an example tweet document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hillary', 'said', 'sniper', 'surrounded', 'usss', 'turned', 'total', 'lie', 'fit', 'to_', 'https', 'hbirgj']\n",
      "\n",
      "Score: 0.9373569488525391\t \n",
      "Topic: 0.005*\"great\" + 0.005*\"https\" + 0.004*\"trump\" + 0.004*\"realdonaldtrump\" + 0.004*\"america\" + 0.003*\"hillary\" + 0.003*\"thank\" + 0.003*\"clinton\" + 0.002*\"enjoy\" + 0.002*\"rt\"\n",
      "\n",
      "Score: 0.01570645347237587\t \n",
      "Topic: 0.005*\"interviewed\" + 0.004*\"https\" + 0.003*\"enjoy\" + 0.003*\"realdonaldtrump\" + 0.003*\"trump\" + 0.003*\"foxandfriends\" + 0.003*\"hillary\" + 0.003*\"president\" + 0.003*\"foxnews\" + 0.003*\"thank\"\n",
      "\n",
      "Score: 0.01568310707807541\t \n",
      "Topic: 0.005*\"https\" + 0.004*\"trump\" + 0.004*\"realdonaldtrump\" + 0.004*\"thank\" + 0.004*\"great\" + 0.003*\"amp\" + 0.003*\"america\" + 0.002*\"people\" + 0.002*\"tonight\" + 0.002*\"cnn\"\n",
      "\n",
      "Score: 0.015653494745492935\t \n",
      "Topic: 0.005*\"https\" + 0.005*\"trump\" + 0.004*\"thank\" + 0.003*\"realdonaldtrump\" + 0.003*\"great\" + 0.003*\"amp\" + 0.002*\"america\" + 0.002*\"people\" + 0.002*\"hillary\" + 0.002*\"donald\"\n",
      "\n",
      "Score: 0.015599987469613552\t \n",
      "Topic: 0.006*\"trump\" + 0.005*\"https\" + 0.005*\"realdonaldtrump\" + 0.004*\"great\" + 0.004*\"thank\" + 0.003*\"poll\" + 0.003*\"http\" + 0.003*\"rt\" + 0.003*\"america\" + 0.002*\"new\"\n"
     ]
    }
   ],
   "source": [
    "# use the same example document no.168 to test tf-idf model performance\n",
    "\n",
    "print(processed_docs[167])\n",
    "\n",
    "for index, score in sorted(lda_model_tfidf[bow_corpus[167]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In general, the model performance is ok given the example tweet. However, this may not directly result from \n",
    "the model tuning but may be due to the homogeneity in the tweets, "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
